[
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Evaluating Model",
    "section": "",
    "text": "cleaned_data = \"../data/splits\"\nlive = Live(dir=\"../eval\", dvcyaml=False, report=None)\nDefine kfold for evaluation\nKFOLD = 1\nLoad Evaluation Dataset\nval_dataset = pd.read_csv(f'{cleaned_data}/val/FAA-{KFOLD}.csv',header=0)\nImport fine-tuned model\nmodel_path = '../model/'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntext = val_dataset.text\npredictions = []\nactual_predictions = []\n\nfor row in text:\n    inputs = tokenizer(row, return_tensors=\"pt\")\n    \n    with torch.no_grad():\n        logits = model(**inputs).logits\n    \n    predictions.append(logits)\n    actual_predictions.append(logits.argmax().item())"
  },
  {
    "objectID": "inference.html#visualizations",
    "href": "inference.html#visualizations",
    "title": "Evaluating Model",
    "section": "Visualizations",
    "text": "Visualizations\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n\nPrediction Heat Maps\nCount correct predictions and add to heat map\n\ncorrect = 0\nheat_map = np.zeros((7,7), dtype=float)\n\nfor index, label in enumerate(val_dataset.label):\n    if label == actual_predictions[index]:\n        correct += 1\n    \n    heat_map[6 - actual_predictions[index]][label] = heat_map[ 6 - actual_predictions[index]][label] + 1\n\nprint(\"Correct based on my actual predictions: \", correct/len(actual_predictions))\n\nSave Metrics\n\nmetrics = {'accuracy' : correct/len(actual_predictions)}\nlive.summary = metrics\nlive.make_summary()\n\nNormalize heat map\n\nfor i, category in enumerate(heat_map):\n    total = 0\n    \n    for val in category:\n        total = total + val\n        \n    for j, val in enumerate(category):\n        heat_map[i][j] = val / total\n\nPlot heat map\n\nfig, ax = plt.subplots(figsize=(11,9))\nfig.set_tight_layout(True)\n# color map\nlabels = ['II','ME','AU','AF','DE','EQ','AI']\ny_labels = ['AI','EQ','DE','AF','AU','ME','II']\nsb.heatmap(heat_map,cmap=\"Blues\",xticklabels=labels, yticklabels=y_labels, annot=True)\n\n\nactual = val_dataset.label.tolist()\npredicted = actual_predictions\nlive.log_sklearn_plot(\"confusion_matrix\", actual, predicted)"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training Model",
    "section": "",
    "text": "cleaned_data = '../data/splits'"
  },
  {
    "objectID": "training.html#preprocessing",
    "href": "training.html#preprocessing",
    "title": "Training Model",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nDefine Parameters\n\nKFOLD = 1\nTOKENIZER: str = \"bert-base-cased\"\nLEARNING_RATE: float = 5e-5\nBATCH_SIZE: int = 8\nEPOCHS: int = 2\n\nRead kfold data into dataset\n\nraw_datasets = load_dataset(\"csv\",data_files={'train': [f'{cleaned_data}/train/FAA-{KFOLD}.csv'], 'test': [f'{cleaned_data}/test/FAA-{KFOLD}.csv'],\n                                                'val': [f'{cleaned_data}/val/FAA-{KFOLD}.csv']})\n\n\nmodel_nm = \"bert-base-cased\"\n\nCreate tokenizer\n\ntokz = AutoTokenizer.from_pretrained(TOKENIZER)\n\nTokenize inputs\n\ndef tok_func(x):\n    return tokz(x[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = raw_datasets.map(tok_func, batched=True)\n\nDefine datasets for training\n\nfull_train_dataset = tokenized_datasets[\"train\"]\nfull_eval_dataset = tokenized_datasets[\"test\"]\nfull_val_dataset = tokenized_datasets[\"val\"]\n\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokz)\n\n\nimport numpy as np\nimport evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)"
  },
  {
    "objectID": "training.html#train-and-evaluate-model",
    "href": "training.html#train-and-evaluate-model",
    "title": "Training Model",
    "section": "Train and Evaluate Model",
    "text": "Train and Evaluate Model\n\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(TOKENIZER, num_labels=7)\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"../output/\",\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=full_train_dataset,\n    eval_dataset=full_eval_dataset,\n    tokenizer=tokz,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nhistory = trainer.train()\n\nSave model\n\ntrainer.save_model(\"../model/\")"
  },
  {
    "objectID": "data_collection.html",
    "href": "data_collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "When starting a new project, we’ll load all raw data into the data/raw-data/ directory. Define the relative path here.\nraw_data = '../data/raw-data'"
  },
  {
    "objectID": "data_collection.html#data",
    "href": "data_collection.html#data",
    "title": "Data Collection",
    "section": "Data",
    "text": "Data\nAll of our data came from https://av-info.faa.gov/dd_sublevel.asp?Folder=%5CAID, which provides text files of flight incident records from 1975-2022 in five year increments. We then converted the text files to CSV files using excel.\nLets first import all datasets from our raw data directory\n\ndataset1 = f'{raw_data}/a1975_79.csv'\ndataset2 = f'{raw_data}/a1980_84.csv'\ndataset3 = f'{raw_data}/a1985_89.csv'\ndataset4 = f'{raw_data}/a1990_94.csv'\ndataset5 = f'{raw_data}/a1995_99.csv'\ndataset6 = f'{raw_data}/a2000_04.csv'\ndataset7 = f'{raw_data}/a2005_09.csv'\ndataset8 = f'{raw_data}/a2010_14.csv'\ndataset9 = f'{raw_data}/a2015_19.csv'\ndataset0 = f'{raw_data}/a2020_25.csv'\n\nWe then convert all daatasets to pandas dataframes\n\ndf1 = pd.read_csv(dataset1, header = 0)\ndf2 = pd.read_csv(dataset2, header = 0)\ndf3 = pd.read_csv(dataset3, header = 0)\ndf4 = pd.read_csv(dataset4, header = 0)\ndf5 = pd.read_csv(dataset5, header = 0)\ndf6 = pd.read_csv(dataset6, header = 0)\ndf7 = pd.read_csv(dataset7, header = 0)\ndf8 = pd.read_csv(dataset8, header = 0)\ndf9 = pd.read_csv(dataset9, header = 0)\ndf0 = pd.read_csv(dataset0, header = 0)\n\n/tmp/ipykernel_439980/4246310983.py:1: DtypeWarning: Columns (3,43,56,57,60,61,67,68,69,70,71,72,73,74,75,76,77,133) have mixed types. Specify dtype option on import or set low_memory=False.\n  df1 = pd.read_csv(dataset1, header = 0)\n/tmp/ipykernel_439980/4246310983.py:2: DtypeWarning: Columns (2,3,58,74,77,80) have mixed types. Specify dtype option on import or set low_memory=False.\n  df2 = pd.read_csv(dataset2, header = 0)\n/tmp/ipykernel_439980/4246310983.py:3: DtypeWarning: Columns (2,3,74,75,80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(dataset3, header = 0)\n/tmp/ipykernel_439980/4246310983.py:4: DtypeWarning: Columns (80,81) have mixed types. Specify dtype option on import or set low_memory=False.\n  df4 = pd.read_csv(dataset4, header = 0)\n/tmp/ipykernel_439980/4246310983.py:5: DtypeWarning: Columns (74,80,81,177,178,180,181,182,183,184,185,188) have mixed types. Specify dtype option on import or set low_memory=False.\n  df5 = pd.read_csv(dataset5, header = 0)\n/tmp/ipykernel_439980/4246310983.py:6: DtypeWarning: Columns (2,3,68,74,80) have mixed types. Specify dtype option on import or set low_memory=False.\n  df6 = pd.read_csv(dataset6, header = 0)\n/tmp/ipykernel_439980/4246310983.py:7: DtypeWarning: Columns (2,3,10,43,59,67,69,70,72,76,111) have mixed types. Specify dtype option on import or set low_memory=False.\n  df7 = pd.read_csv(dataset7, header = 0)\n/tmp/ipykernel_439980/4246310983.py:8: DtypeWarning: Columns (5,6,7,8,9,22,29,30,42,58,59,62,63,66,76,81,177,178,180,183) have mixed types. Specify dtype option on import or set low_memory=False.\n  df8 = pd.read_csv(dataset8, header = 0)\n/tmp/ipykernel_439980/4246310983.py:9: DtypeWarning: Columns (5,6,7,8,9,22,29,30,42,58,59,62,63,66,178,182) have mixed types. Specify dtype option on import or set low_memory=False.\n  df9 = pd.read_csv(dataset9, header = 0)\n/tmp/ipykernel_439980/4246310983.py:10: DtypeWarning: Columns (5,6,7,8,9,22,29,30,42,58,59,62,63,66,178,181,182,183) have mixed types. Specify dtype option on import or set low_memory=False.\n  df0 = pd.read_csv(dataset0, header = 0)\n\n\nAs you can see, this data is not clean, we’ll do that in our next step\nFirst, concat all data into one dataframe to easlit work with all the data.\n\ndf = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df0], ignore_index=True)\n\n\ndf\n\n\n\n\n\n\n\n\nc5\nc1\nc2\nc3\nc4\nc6\nc7\nc8\nc9\nc10\n...\n32\nUnnamed: 180\nUnnamed: 181\nUnnamed: 182\nUnnamed: 183\nUnnamed: 184\nUnnamed: 185\nUnnamed: 186\nUnnamed: 187\nUnnamed: 188\n\n\n\n\n0\n19750101000049A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n19750101000129A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n19750101000139A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n19750101000219A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n19750101000229A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n215303\n20220207001489I\nI\n91\n\n\n2022\n2\n7\n20220207\n1505\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n215304\n20220206001469I\nI\n91\n\n\n2022\n2\n6\n20220206\n930\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n215305\n20220206001479I\nI\n91\n\n\n2022\n2\n6\n20220206\n1710\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n215306\n20220131001459I\nI\nO\n\n\n2022\n1\n31\n20220131\n832\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n215307\n20211118022049A\nA\n91\n\n\n2021\n11\n18\n20211118\n1130\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n215308 rows × 190 columns\n\n\n\nThen, save thte concatenated dataframe to our working-data/ directory.\n\ndf.to_csv(f'../data/Concatenated_Orig_data.csv')\n\n\ndf.shape\n\n(215308, 190)"
  },
  {
    "objectID": "feature_extraction.html",
    "href": "feature_extraction.html",
    "title": "Feature Extraction",
    "section": "",
    "text": "At this point our workflow will split as we prepare for different modeling techniques. This notebook will preprocess the data for Neural Net training and inference in 03a_Training_Model.ipynb\nFirst let’s import our cleaned concatenated data from 01_Cleaning_Data.ipynb\ndf = pd.read_csv(\"../data/Concatenated_Clean_data.csv\")\n\n/tmp/ipykernel_440158/2483919307.py:1: DtypeWarning: Columns (5,6,11,15,16,17,18,19,27,29,30,33,34,35,36,43,45,48,49,50,51,52,58,59,60,61,62,63,69,70,71,72,73,74,75,76,77,78,79,82,83,117,118,122,123,127,131,135,136,142,155,158,167,168,172,173,175,176) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"../data/Concatenated_Clean_data.csv\")"
  },
  {
    "objectID": "feature_extraction.html#feature-extraction",
    "href": "feature_extraction.html#feature-extraction",
    "title": "Feature Extraction",
    "section": "Feature Extraction",
    "text": "Feature Extraction\nIn this study, 8 maintenance codes were observed as relevant. We’ll extract those now\n\nmaintenance_codes = ['AF', 'DE', 'AI', 'AP', 'AU', 'EQ', 'II', 'ME']\n\n\ndf = df[df['c78'].isin(maintenance_codes)]\ndf['c78'].value_counts()\n\nII    1951\nME     377\nAU     246\nAF      92\nDE      57\nEQ      24\nAI      15\nAP       1\nName: c78, dtype: int64\n\n\nNext, we identify and select relevant data and label columns\n\ntext_columns = ['c119','c77','c79','c81', 'c85', 'c87', 'c89', 'c91', 'c93', 'c95', 'c97', 'c99', 'c101', 'c103', 'c105', 'c107', 'c109', 'c131', 'c133', 'c135', 'c137', 'c146', 'c148', 'c150', 'c154','c161', 'c163', 'c183', 'c191']\nlabel_columns = ['c78', 'c80', 'c86', 'c5']\n\ncolumns_to_keep = text_columns + label_columns\ndf.drop(columns=[col for col in df if col not in columns_to_keep], inplace=True)\n\n\ndf.shape\n\n(2763, 33)\n\n\nThis is our maintenence text csv, we’ll save that now\n\ndf.to_csv(\"../data/cleaned-data/Maintenance_Text_data.csv\")\n\nFor our NLP classification, we only need two columns: c119 is the text that describes an issue, c78 is the label that classifies the issue\nWe’ll extract those now\n\ndata = pd.DataFrame()\ndata['text'] = df['c119']\ndata['label'] = df['c78']\ndata\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n535\nTAILWHEEL COCKED RIGHT PRIOR TO TKOF. ...\nAU\n\n\n864\nTOW PLANE BECAME AIRBORNE THEN SETTLED.STUDENT...\nME\n\n\n2195\n2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...\nAU\n\n\n2476\nPLT NOTED SOFT R BRAKE PEDAL DRG TAXI TO TKOF....\nAU\n\n\n2916\nTAXI OFF HARD SFC DUE TFC R MAIN GR BROKE THRO...\nAF\n\n\n...\n...\n...\n\n\n113835\n(-23) A/C RELOCATED TO NEW HANGAR TO CHECK SIZ...\nII\n\n\n113838\n(-23) ON 2/23/08 @ APPROXIMATELY 2130 DURING T...\nAF\n\n\n113840\n(-23) PILOT TOOK OFF FOR LEESBURG AIRPORT AND ...\nII\n\n\n113869\n(-23) OWNER FORGOT TO FASTEN THE LOWER LEFT 4 ...\nII\n\n\n113902\n(-23) THE AIRCRAFT EXPERIENCED SEVERE TURBULAN...\nME\n\n\n\n\n2763 rows × 2 columns"
  },
  {
    "objectID": "feature_extraction.html#cleaning-dataframe",
    "href": "feature_extraction.html#cleaning-dataframe",
    "title": "Feature Extraction",
    "section": "Cleaning Dataframe",
    "text": "Cleaning Dataframe\nEven with previous cleaning, lets ensure our dataframe is clean\n\ndata.isna().sum()\n\ntext     15\nlabel     0\ndtype: int64\n\n\nRemove NaN values\n\ndata = data.fillna('Null')\ndata = data[data['text'] != 'Null']\n\nCheck there are no missing values left\n\ndata.isna().sum()\n\ntext     0\nlabel    0\ndtype: int64\n\n\n\ndata.head(10)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n535\nTAILWHEEL COCKED RIGHT PRIOR TO TKOF. ...\nAU\n\n\n864\nTOW PLANE BECAME AIRBORNE THEN SETTLED.STUDENT...\nME\n\n\n2195\n2ND ILS APCH,ACFT'S G/S INOP.LOM TUNED TO WRON...\nAU\n\n\n2476\nPLT NOTED SOFT R BRAKE PEDAL DRG TAXI TO TKOF....\nAU\n\n\n2916\nTAXI OFF HARD SFC DUE TFC R MAIN GR BROKE THRO...\nAF\n\n\n3151\nACFT BEING TAXIED ON GRASS TAXIWAY NOSE WHEEL ...\nAF\n\n\n3332\nDEP FOR DEST WITH KNOWN ELEC PROB. DIDNT USE E...\nAU\n\n\n3943\nMTNS OBSCURED.FLT TO CK VOR REC REPTD INOP PRI...\nAU\n\n\n4176\nSUFFICIENT OPPORTUNITY EXISTED TO RELEASE WHEN...\nME\n\n\n4442\nMAINT NOT PERFORMED DUE PARTS NOT AVAILABLE. T...\nAU\n\n\n\n\n\n\n\nRemove rows with one occurance\n\ncounts = data['label'].value_counts()\ndata = data[data['label'].isin(counts[counts &gt; 1].index)]"
  },
  {
    "objectID": "feature_extraction.html#splitting-data",
    "href": "feature_extraction.html#splitting-data",
    "title": "Feature Extraction",
    "section": "Splitting Data",
    "text": "Splitting Data\n\nX, y = data['text'], data['label']\n\nWe’ll split data into training (60%), testing (20%), and validating (20%)\n\nss = StratifiedShuffleSplit(n_splits=10, test_size=0.20, random_state=0)\nss.get_n_splits(X, y)\n\n10\n\n\n\noutput_dir = '../data/splits'\n\nCreate Output Dirs\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nif not os.path.exists(output_dir + '/train'):\n    os.makedirs(output_dir + '/train')\n\nif not os.path.exists(output_dir + '/test'):\n    os.makedirs(output_dir + '/test')\n\nif not os.path.exists(output_dir + '/val'):\n    os.makedirs(output_dir + '/val')\n\nif not os.path.exists(output_dir + '/actual'):\n    os.makedirs(output_dir + '/actual')\n\n\nfor i, (train_index, test_index) in enumerate(ss.split(X, y)):\n    X_train , X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train , y_test = y.iloc[train_index] , y.iloc[test_index]\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.20, random_state=0)\n    \n    Encoder = LabelEncoder()\n    y_train = Encoder.fit_transform(y_train)\n    y_test = Encoder.fit_transform(y_test)\n    y_val_encode = Encoder.fit_transform(y_val)\n    \n    final_train = pd.DataFrame({'text':X_train,'label':y_train})\n    final_test = pd.DataFrame({'text':X_test,'label':y_test})\n    final_val = pd.DataFrame({'text':X_val,'label':y_val_encode})\n    \n    final_train.to_csv(f'{output_dir}/train/FAA-{i}.csv', index=False)\n    final_test.to_csv(f'{output_dir}/test/FAA-{i}.csv', index=False)\n    final_val.to_csv(f'{output_dir}/val/FAA-{i}.csv', index=False)\n    y_val.to_csv(f'{output_dir}/actual/FAA-{i}.csv', index=False)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nbdev-framework-example",
    "section": "",
    "text": "PDM, as described, is a modern Python package and dependency manager supporting the latest PEP standards. But it is more than a package manager. It boosts your development workflow in various aspects. The most significant benefit is it installs and manages packages in a similar way to npm that doesn’t need to create a virtualenv at all!\n— PDM Documentation\n\n\n\nPDM should be installed as described in the Installation instructions.\n\n\n\nOnce PDM is installed and configured, the project should be initialized by running the following command. This will ask a series of questions, where the defaults are usually safe, and produce a file called pyproject.toml.\npdm init\n\n\n\n\n\nDVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\n— DVC Website\n\n\n\nAs we are using PDM for managing python dependencies of the project, we are able to add DVC by running pdm add dvc. This will add it to the pyproject.toml file and install it into the environment.\n\n\n\nTo add DVC to the project, we need to do a one time initialization by running pdm run dvc init. This will generate a .dvc directory which stores it’s internal files as well as a files to DVC what files it should ignore.\n\n\n\n\n\nnbdev is a notebook-driven development platform. Simply write notebooks with lightweight markup and get high-quality documentation, tests, continuous integration, and packaging for free!\nnbdev Website\n\n\n\nAs we are using nbdev for creating documentation automatically, we are able to add nbdev by running pdm add nbdev. This will add it to the pyproject.toml file and install it into the environment.\n\n\n\nTo add nbdev to the project, we need to do a one time initialzation by running pdm run nbdev_new. This will create a settings.ini file with project settings, a .github/workflows directory containing workflows for package building and documentation deploying, and a nbs directory containing jupyter notebooks for project files. These notebooks will be deployed as documentation pages using Quarto\n\n\n\nIf you are using nbdev for documentation only without the package creation feature you can follow these steps: 1. Remove library files\nrm setup.py .github/workflows/test.yaml nbs/00_core.ipynb\n\nRemove you library folder (this will be the lib_path field in settings.ini)\n\nrm -rf &lt;lib_path&gt;"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "nbdev-framework-example",
    "section": "",
    "text": "PDM, as described, is a modern Python package and dependency manager supporting the latest PEP standards. But it is more than a package manager. It boosts your development workflow in various aspects. The most significant benefit is it installs and manages packages in a similar way to npm that doesn’t need to create a virtualenv at all!\n— PDM Documentation\n\n\n\nPDM should be installed as described in the Installation instructions.\n\n\n\nOnce PDM is installed and configured, the project should be initialized by running the following command. This will ask a series of questions, where the defaults are usually safe, and produce a file called pyproject.toml.\npdm init\n\n\n\n\n\nDVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\n— DVC Website\n\n\n\nAs we are using PDM for managing python dependencies of the project, we are able to add DVC by running pdm add dvc. This will add it to the pyproject.toml file and install it into the environment.\n\n\n\nTo add DVC to the project, we need to do a one time initialization by running pdm run dvc init. This will generate a .dvc directory which stores it’s internal files as well as a files to DVC what files it should ignore.\n\n\n\n\n\nnbdev is a notebook-driven development platform. Simply write notebooks with lightweight markup and get high-quality documentation, tests, continuous integration, and packaging for free!\nnbdev Website\n\n\n\nAs we are using nbdev for creating documentation automatically, we are able to add nbdev by running pdm add nbdev. This will add it to the pyproject.toml file and install it into the environment.\n\n\n\nTo add nbdev to the project, we need to do a one time initialzation by running pdm run nbdev_new. This will create a settings.ini file with project settings, a .github/workflows directory containing workflows for package building and documentation deploying, and a nbs directory containing jupyter notebooks for project files. These notebooks will be deployed as documentation pages using Quarto\n\n\n\nIf you are using nbdev for documentation only without the package creation feature you can follow these steps: 1. Remove library files\nrm setup.py .github/workflows/test.yaml nbs/00_core.ipynb\n\nRemove you library folder (this will be the lib_path field in settings.ini)\n\nrm -rf &lt;lib_path&gt;"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Cleaning Data",
    "section": "",
    "text": "We will be using the Concatenated_Orig_data.csv, that was created using the 00_Collecting_Data.ipynb\ndf = pd.read_csv(f\"../data/Concatenated_Orig_data.csv\",encoding='latin-1', header=0)\n\n/tmp/ipykernel_440135/286995959.py:1: DtypeWarning: Columns (3,4,6,7,8,9,10,11,23,30,31,43,44,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,81,82,104,106,111,112,113,134,178,179,182,183,184,185,186,187,190) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(f\"../data/Concatenated_Orig_data.csv\",encoding='latin-1', header=0)"
  },
  {
    "objectID": "data_cleaning.html#preprocessing",
    "href": "data_cleaning.html#preprocessing",
    "title": "Cleaning Data",
    "section": "Preprocessing",
    "text": "Preprocessing\nFirst, lets examine our data:\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nc5\nc1\nc2\nc3\nc4\nc6\nc7\nc8\nc9\n...\n32\nUnnamed: 180\nUnnamed: 181\nUnnamed: 182\nUnnamed: 183\nUnnamed: 184\nUnnamed: 185\nUnnamed: 186\nUnnamed: 187\nUnnamed: 188\n\n\n\n\n0\n0\n19750101000049A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1\n19750101000129A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2\n19750101000139A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n3\n19750101000219A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n4\n19750101000229A\nA\n\n0.4\n\n1975\n1\n1\n19750101\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 191 columns\n\n\n\nLet’s clean the data set by dropping duplicate entries by the identification number and converting blank cells to NaN\n\ndf = df.drop_duplicates(subset=['c5'])\ndf.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
  },
  {
    "objectID": "data_cleaning.html#selecting-opcodes",
    "href": "data_cleaning.html#selecting-opcodes",
    "title": "Cleaning Data",
    "section": "Selecting Opcodes",
    "text": "Selecting Opcodes\nNow, let’s get a better sense of the target column c78. Specifcally, we’d like to know how many instances there are of each label, and limit ourselves to only the most frequent labels to reduce numerosity.\nLet’s limit the data to those entries with the value of c78 appearing in the AIDCODE.csv\n\ndf2 = pd.read_csv(\"../data/raw-data/AIDCODE.csv\")\nlist_code = df2['CODE'].unique()\nlist_code = list_code.tolist()\n\n\n#drop rows that contain any value in the list\ndf = df[df.c78.isin(list_code) == True]\n\n\ndf.dropna(subset=['c78'],inplace=True)\n\n\ndf['c78'].value_counts()\n\nGC    14100\nLO     9009\nHO     8832\nGN     6085\nAS     5800\n      ...  \nFC        3\nIF        1\nFE        1\nAP        1\nCS        1\nName: c78, Length: 115, dtype: int64\n\n\n\ndf.shape[0]\n\n113942"
  },
  {
    "objectID": "data_cleaning.html#saving",
    "href": "data_cleaning.html#saving",
    "title": "Cleaning Data",
    "section": "Saving",
    "text": "Saving\n\ndf.to_csv(\"../data/Concatenated_Clean_data.csv\")"
  },
  {
    "objectID": "example.html",
    "href": "example.html",
    "title": "Example",
    "section": "",
    "text": "Here’s a single example of our workflow using the NN model\nLet’s pull a random entry from our datasets\n\ntext = 'TOOK OFF WITH ONE GENERATOR INOPERATIVE, THE OTHER THEN FAILED, CAUSING INSUFFICIENT POWER TO LOCK LANDING GEAR.'\nlabel = 'AU'\nlabel_encoded = 2\n\nThis text description corresponds with lablel AU which is encoded as 2\nFirst we tokenize our imput\n\nfrom transformers import AutoTokenizer\n\nmodel_path = '../model/'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n2023-03-01 20:25:49.841942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ninputs = tokenizer(text, return_tensors='pt')\n\nNow we pass the inputs to the model\n\nfrom transformers import AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\nLets check our results\n\nlogits.argmax().item()\n\n5\n\n\nAs we can see our model is incorrect"
  }
]