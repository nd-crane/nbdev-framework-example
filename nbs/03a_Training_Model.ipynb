{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee708c-071b-4e87-b170-8b9595face42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# This notebook is an outline for 10 fold cross validation neural network\n",
    "# Environment Setup\n",
    "#! pdm add transformers\n",
    "#! pdm add datasets\n",
    "#! pdm add keras==2.6.*\n",
    "#! pdm add torch==1.8.0 torchtext==0.9.0\n",
    "#! pdm add torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e159a8-6d54-4531-85bb-e37b68f6022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/afs/crc.nd.edu/group/TAI/Users/painswor/nbdev-framework-example/nbs', '/opt/anaconda3/lib/python39.zip', '/opt/anaconda3/lib/python3.9', '/opt/anaconda3/lib/python3.9/lib-dynload', '', '/afs/crc.nd.edu/user/p/painswor/.local/lib/python3.9/site-packages', '/opt/anaconda3/lib/python3.9/site-packages', '/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/afs/crc.nd.edu/user/p/painswor/.ipython', '../__pypackages__/3.9/lib/', '../__pypackages__/3.9/lib/', '../__pypackages__/3.9/lib/']\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append('../__pypackages__/3.9/lib/')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa79e6c-d874-487d-9e07-a0a0281047f3",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e5bd7-9a94-487f-9e49-1019a40738bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = '../data/processed-data/nn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858684b-d1da-404b-8ded-a0b22e1bda81",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff35846-b000-4c50-b86c-2bb5ce757098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from datasets import Dataset,DatasetDict,load_dataset\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438261d3-7ed3-427b-8b87-7f6ad8247847",
   "metadata": {},
   "source": [
    "Set kfold to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b149ba-671d-4e24-8cce-86bc3dcf7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e4aed-c2cb-4831-a668-cc6d3e758e81",
   "metadata": {},
   "source": [
    "Read kfold data into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786dcac-76df-4cfa-a9b4-050747a9354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b9c4db56f9195e16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/datasets/csv/default-b9c4db56f9195e16/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3819f49f83794a6498c174b36d6245af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef605fd27fd34b2185fdc75478a6aa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/datasets/csv/default-b9c4db56f9195e16/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71877ce22d74f07b07c788ac4d50420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"csv\",data_files={'train': [f'{cleaned_data}/train/FAA-{kfold}.csv'], 'test': [f'{cleaned_data}/test/FAA-{kfold}.csv'],\n",
    "                                                'val': [f'{cleaned_data}/val/FAA-{kfold}.csv']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f01750-e09c-4253-a2a2-684e719b33e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1757\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 550\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e6d95-a996-4702-a06b-21fa45d428e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'WIND RAISED LIGHT SLING LOAD   CABLE HIT TAILROTOR  MADE HARD AUTOROTATE LANDING',\n",
       " 'label': 6}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100cc6a-17a2-4e11-8a05-219f2791a8d1",
   "metadata": {},
   "source": [
    "Tokenize text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901a18d-594c-4ca9-82b3-60c74aea77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bf120-a592-494e-8881-9a1cd07b6626",
   "metadata": {},
   "source": [
    "Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6ba0f-e2d4-4e67-9605-147cc9dce617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb41ad-7473-4699-8db0-3580baca8546",
   "metadata": {},
   "source": [
    "Tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c182a10-dea3-4caa-be8c-aaeda824793b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeac0e115d8e4195a476f888e2bfa83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3831873143944b138cafb4a7145c30cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cd2c56990648729afba616a85b147c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tok_func(x):\n",
    "    return tokz(x[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cc496-a849-4543-914a-56b00a5bb1ca",
   "metadata": {},
   "source": [
    "Define datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d4d5a-4110-4495-baee-804c07ada651",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]\n",
    "full_val_dataset = tokenized_datasets[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c94ef-1f8f-484a-9e6b-0894076b6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c5257-0a50-426f-b3c1-78a2aef48488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed495aa-18ad-4117-adbe-864c6bf4ebf7",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae0117-7bb9-44c4-b452-200016fa6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62b67b-6f65-4349-a443-68c7e939f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /afs/crc.nd.edu/user/p/painswor/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cf2aa-15a4-4063-990c-4a7c0bdaadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../output/\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240002b8-631b-4de9-93a1-aa2e7cda076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    tokenizer=tokz,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64004735-a6e7-4780-be8b-5d9a72ea51bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/afs/crc.nd.edu/group/TAI/Users/painswor/nbdev-framework-example/nbs/../__pypackages__/3.9/lib/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1757\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 110\n",
      "  Number of trainable parameters = 108315655\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/afs/crc.nd.edu/group/TAI/Users/painswor/nbdev-framework-example/nbs/../__pypackages__/3.9/lib/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 01:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.012146</td>\n",
       "      <td>0.707273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.977870</td>\n",
       "      <td>0.707273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 550\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../output/checkpoint-55\n",
      "Configuration saved in ../output/checkpoint-55/config.json\n",
      "Model weights saved in ../output/checkpoint-55/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-55/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-55/special_tokens_map.json\n",
      "/afs/crc.nd.edu/group/TAI/Users/painswor/nbdev-framework-example/nbs/../__pypackages__/3.9/lib/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 550\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ../output/checkpoint-110\n",
      "Configuration saved in ../output/checkpoint-110/config.json\n",
      "Model weights saved in ../output/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in ../output/checkpoint-110/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../output/checkpoint-110 (score: 0.977869987487793).\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da38cb1-4b47-4501-bedf-9ee2122afe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../output/model\n",
      "Configuration saved in ../output/model/config.json\n",
      "Model weights saved in ../output/model/pytorch_model.bin\n",
      "tokenizer config file saved in ../output/model/tokenizer_config.json\n",
      "Special tokens file saved in ../output/model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"../output/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
