{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee708c-071b-4e87-b170-8b9595face42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from dvclive import Live\n",
    "from dvclive.huggingface import DVCLiveCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa79e6c-d874-487d-9e07-a0a0281047f3",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e5bd7-9a94-487f-9e49-1019a40738bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = '../data/splits'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858684b-d1da-404b-8ded-a0b22e1bda81",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "438261d3-7ed3-427b-8b87-7f6ad8247847",
   "metadata": {},
   "source": [
    "### Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b149ba-671d-4e24-8cce-86bc3dcf7399",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "KFOLD = 1\n",
    "TOKENIZER: str = \"bert-base-cased\"\n",
    "LEARNING_RATE: float = 5e-5\n",
    "BATCH_SIZE: int = 8\n",
    "EPOCHS: int = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e4aed-c2cb-4831-a668-cc6d3e758e81",
   "metadata": {},
   "source": [
    "Read kfold data into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786dcac-76df-4cfa-a9b4-050747a9354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"csv\",data_files={'train': [f'{cleaned_data}/train/FAA-{KFOLD}.csv'], 'test': [f'{cleaned_data}/test/FAA-{KFOLD}.csv'],\n",
    "                                                'val': [f'{cleaned_data}/val/FAA-{KFOLD}.csv']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901a18d-594c-4ca9-82b3-60c74aea77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bf120-a592-494e-8881-9a1cd07b6626",
   "metadata": {},
   "source": [
    "Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6ba0f-e2d4-4e67-9605-147cc9dce617",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb41ad-7473-4699-8db0-3580baca8546",
   "metadata": {},
   "source": [
    "Tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c182a10-dea3-4caa-be8c-aaeda824793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x):\n",
    "    return tokz(x[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cc496-a849-4543-914a-56b00a5bb1ca",
   "metadata": {},
   "source": [
    "Define datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d4d5a-4110-4495-baee-804c07ada651",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]\n",
    "full_val_dataset = tokenized_datasets[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c94ef-1f8f-484a-9e6b-0894076b6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c5257-0a50-426f-b3c1-78a2aef48488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed495aa-18ad-4117-adbe-864c6bf4ebf7",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae0117-7bb9-44c4-b452-200016fa6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62b67b-6f65-4349-a443-68c7e939f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(TOKENIZER, num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cf2aa-15a4-4063-990c-4a7c0bdaadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../output/\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    tokenizer=tokz,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd972e67",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37864fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
